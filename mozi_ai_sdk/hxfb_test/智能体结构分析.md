# 强化学习智能体结构分析

## 一、项目整体架构

### 1.1 技术栈
- **强化学习框架**: Ray RLlib
- **算法**: PPO (Proximal Policy Optimization) / DDPPO (Distributed PPO)
- **深度学习框架**: PyTorch
- **环境**: 自定义多智能体环境 (MultiAgentEnv)
- **应用场景**: 军事仿真（墨子系统 - 华戍防务）

### 1.2 项目结构
```
hxfb_test/
├── main_train.py          # 训练脚本
├── main_versus.py         # 评估/对战脚本
├── envs/
│   ├── env_hxfb.py       # 核心环境类 HXFBEnv
│   ├── env.py            # 基础环境类
│   └── common/           # 工具函数
├── checkpoint/            # 模型检查点
└── utils/                # 工具模块
```

## 二、核心组件分析

### 2.1 环境定义 (HXFBEnv)

**继承关系**: `MultiAgentEnv` (Ray RLlib的多智能体环境基类)

**关键特性**:
- **观察空间**: `Dict({"obs": Box(-inf, inf, shape=(350,))})`
  - 350维连续观察向量
  - 包含战场状态、单位信息等
  
- **动作空间**: `Discrete(48)`
  - 48个离散动作
  - 包括：无动作、防御性空战任务、反舰任务等

- **奖励机制**: 
  - 基于战场得分: `reward = side.iTotalScore / 4067`
  - 支持累积奖励计算

**环境接口**:
```python
class HXFBEnv(MultiAgentEnv):
    def __init__(self, env_config)      # 初始化环境
    def reset(self)                      # 重置环境
    def step(self, action)               # 执行动作
    def get_observation()                # 获取观察
    def get_reward()                     # 计算奖励
```

### 2.2 训练配置 (main_train.py)

**核心配置参数**:

1. **网络结构**:
   ```python
   "model": {
       "use_lstm": True,                    # 使用LSTM处理序列
       "max_seq_len": 64,                   # 序列最大长度
       "lstm_cell_size": 256,               # LSTM单元大小
       "lstm_use_prev_action_reward": True,  # 将前一步动作和奖励输入LSTM
   }
   ```

2. **PPO超参数**:
   ```python
   "clip_param": 0.3,              # PPO裁剪参数
   "vf_clip_param": 10.0,          # 价值函数裁剪
   "lambda": 0.98,                 # GAE lambda
   "vf_loss_coeff": 1.0,           # 价值函数损失系数
   "kl_coeff": 0.2,                # KL散度系数
   "entropy_coeff": 0.0,           # 熵系数
   "lr": tune.uniform(5e-6, 5e-5), # 学习率范围
   ```

3. **训练参数**:
   ```python
   "num_workers": 0,                    # 并行worker数量
   "num_envs_per_worker": 1,            # 每个worker的环境数
   "rollout_fragment_length": 512,      # 每次rollout的步数
   "train_batch_size": -1,              # 训练批次大小（-1表示自动）
   "sgd_minibatch_size": 128,           # SGD小批次大小
   "num_sgd_iter": 100,                 # SGD迭代次数
   "batch_mode": "truncate_episodes",   # 批次模式
   ```

4. **多智能体配置**:
   ```python
   "multiagent": {
       "agent_0": (obs_space, act_space, {"gamma": 0.99})
   }
   ```

### 2.3 评估/对战脚本 (main_versus.py)

**核心功能**:
1. **模型加载**: 从checkpoint恢复训练好的模型
2. **Rollout执行**: 运行评估episode
3. **LSTM状态管理**: 维护LSTM的隐藏状态
4. **动作计算**: 使用`agent.compute_action()`获取动作

**关键流程**:
```python
# 1. 初始化环境
env = HXFBEnv(env_config)

# 2. 创建并恢复训练器
agent = PPOTrainer(env=env, config=config)
agent.restore(checkpoint)

# 3. 执行rollout
rollout(agent, env, evaluate_episodes, platform_mode)
```

## 三、智能体架构详解

### 3.1 网络结构

**Actor-Critic架构**:
- **共享层**: `vf_share_layers=True` - 价值函数和策略网络共享底层
- **LSTM层**: 256单元，处理时序信息
- **输入**: 
  - 当前观察 (350维)
  - 前一步动作
  - 前一步奖励
- **输出**:
  - 动作概率分布 (48维)
  - 状态价值估计

### 3.2 训练流程

1. **数据收集** (Rollout):
   - 每个worker并行收集经验
   - 使用当前策略与环境交互
   - 收集 (obs, action, reward, next_obs, done)

2. **经验处理**:
   - 计算GAE (Generalized Advantage Estimation)
   - 使用lambda=0.98进行优势估计

3. **策略更新**:
   - 使用PPO裁剪目标函数
   - 多轮SGD更新 (num_sgd_iter=100)
   - 小批次训练 (sgd_minibatch_size=128)

### 3.3 LSTM状态管理

**关键点**:
- LSTM需要维护隐藏状态和细胞状态
- 在episode开始时重置状态
- 在episode内保持状态连续性
- 使用`agent_states`字典存储每个智能体的状态

## 四、如何构建类似案例

### 4.1 环境开发步骤

#### 步骤1: 定义环境类
```python
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from gym.spaces import Discrete, Box, Dict

class YourCustomEnv(MultiAgentEnv):
    def __init__(self, env_config):
        # 1. 定义观察空间
        self.observation_space = Dict({
            "obs": Box(-inf, inf, shape=(obs_dim,))
        })
        
        # 2. 定义动作空间
        self.action_space = Discrete(action_dim)
        
        # 3. 初始化环境状态
        self.reset()
    
    def reset(self):
        # 重置环境到初始状态
        # 返回初始观察
        obs = self._get_observation()
        return obs
    
    def step(self, action):
        # 1. 执行动作
        # 2. 更新环境状态
        # 3. 计算奖励
        # 4. 检查是否结束
        # 5. 获取新观察
        obs = self._get_observation()
        reward = self._calculate_reward()
        done = self._check_done()
        info = {}
        return obs, reward, done, info
    
    def _get_observation(self):
        # 将环境状态转换为观察向量
        pass
    
    def _calculate_reward(self):
        # 根据环境状态计算奖励
        pass
```

#### 步骤2: 设计观察空间
- 确定观察维度
- 归一化观察值
- 处理缺失信息（填充或mask）

#### 步骤3: 设计动作空间
- 离散动作：定义所有可能的动作
- 连续动作：定义动作范围和维度

#### 步骤4: 设计奖励函数
- 稀疏奖励 vs 密集奖励
- 奖励塑形 (Reward Shaping)
- 多目标奖励组合

### 4.2 训练脚本开发

#### 步骤1: 配置训练参数
```python
from ray.rllib.agents.ppo import PPOTrainer
from gym.spaces import Discrete, Box, Dict

# 定义空间
act_space = Discrete(action_size)
obs_space = Dict({"obs": Box(-inf, inf, shape=(obs_size,))})

# 配置
config = {
    "env": YourCustomEnv,
    "env_config": {
        "mode": "train",
        # 其他环境参数
    },
    "framework": "torch",
    "model": {
        "use_lstm": True,
        "max_seq_len": 64,
        "lstm_cell_size": 256,
        "lstm_use_prev_action_reward": True,
    },
    "multiagent": {
        "agent_0": (obs_space, act_space, {"gamma": 0.99})
    },
    "num_workers": 4,
    "num_envs_per_worker": 1,
    "rollout_fragment_length": 512,
    "train_batch_size": 4000,
    "sgd_minibatch_size": 128,
    "num_sgd_iter": 30,
    "lr": 3e-4,
    "lambda": 0.95,
    "clip_param": 0.3,
    "vf_clip_param": 10.0,
}
```

#### 步骤2: 训练循环
```python
import ray
from ray import tune

ray.init()

# 使用tune进行超参数搜索
results = tune.run(
    "PPO",
    config=config,
    num_samples=1,
    checkpoint_freq=10,
    local_dir="./results"
)
```

### 4.3 评估脚本开发

#### 步骤1: 加载模型
```python
from ray.rllib.agents.ppo import PPOTrainer

agent = PPOTrainer(env=YourCustomEnv, config=config)
agent.restore(checkpoint_path)
```

#### 步骤2: 执行评估
```python
def rollout(agent, env, num_episodes):
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        agent_states = {}  # LSTM状态
        
        while not done:
            # 计算动作（带LSTM状态）
            action, agent_states, _ = agent.compute_action(
                obs,
                state=agent_states,
                explore=False  # 评估时不探索
            )
            
            # 执行动作
            obs, reward, done, info = env.step(action)
            total_reward += reward
        
        print(f"Episode {episode}: Reward = {total_reward}")
```

### 4.4 关键注意事项

1. **观察归一化**:
   - 确保观察值在合理范围内
   - 考虑使用归一化或标准化

2. **奖励设计**:
   - 奖励应该与目标一致
   - 避免奖励稀疏导致学习困难
   - 考虑奖励塑形

3. **超参数调优**:
   - 学习率：通常从3e-4开始
   - 批次大小：根据内存和计算资源调整
   - LSTM参数：根据任务复杂度调整

4. **调试技巧**:
   - 先在小规模环境测试
   - 监控训练指标（loss, reward等）
   - 可视化观察和动作分布

5. **分布式训练**:
   - 设置合适的num_workers
   - 注意数据同步和通信开销

## 五、完整示例模板

### 5.1 简单环境示例
```python
import numpy as np
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from gym.spaces import Discrete, Box, Dict

class SimpleEnv(MultiAgentEnv):
    def __init__(self, env_config):
        self.obs_dim = env_config.get("obs_dim", 10)
        self.action_dim = env_config.get("action_dim", 5)
        self.max_steps = 100
        
        self.observation_space = Dict({
            "obs": Box(-1.0, 1.0, shape=(self.obs_dim,))
        })
        self.action_space = Discrete(self.action_dim)
        
        self.reset()
    
    def reset(self):
        self.step_count = 0
        self.state = np.random.randn(self.obs_dim)
        return {"obs": self.state}
    
    def step(self, action):
        # 简单的状态转移
        self.state = self.state + 0.1 * np.random.randn(self.obs_dim)
        self.state = np.clip(self.state, -1, 1)
        
        # 简单奖励：鼓励某些动作
        reward = 1.0 if action == 0 else -0.1
        
        self.step_count += 1
        done = self.step_count >= self.max_steps
        
        return {"obs": self.state}, reward, done, {}
```

### 5.2 训练脚本模板
```python
import ray
from ray import tune
from ray.rllib.agents.ppo import PPOTrainer
from gym.spaces import Discrete, Box, Dict
from your_env import SimpleEnv

ray.init()

obs_space = Dict({"obs": Box(-1.0, 1.0, shape=(10,))})
act_space = Discrete(5)

config = {
    "env": SimpleEnv,
    "env_config": {
        "obs_dim": 10,
        "action_dim": 5,
    },
    "framework": "torch",
    "model": {
        "use_lstm": True,
        "lstm_cell_size": 128,
        "max_seq_len": 32,
    },
    "multiagent": {
        "agent_0": (obs_space, act_space, {"gamma": 0.99})
    },
    "num_workers": 2,
    "rollout_fragment_length": 200,
    "train_batch_size": 4000,
    "sgd_minibatch_size": 64,
    "num_sgd_iter": 30,
    "lr": 3e-4,
}

tune.run("PPO", config=config, num_samples=1)
```

## 六、总结

这个项目的核心特点：
1. **使用Ray RLlib框架** - 成熟的分布式RL框架
2. **PPO算法** - 稳定的策略梯度方法
3. **LSTM网络** - 处理时序依赖
4. **多智能体支持** - 可扩展的架构
5. **完整的训练-评估流程** - 生产就绪的代码结构

构建类似项目时，重点关注：
- 环境接口的正确实现
- 观察和动作空间的设计
- 奖励函数的设计
- 超参数的调优
- 训练和评估流程的完善

