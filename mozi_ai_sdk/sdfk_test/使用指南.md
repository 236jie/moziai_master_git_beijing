# 训练使用指南

## 快速开始

### 1. 启动训练

```bash
python main_train.py --platform_mode eval --training_iteration 1000
```

### 2. 实时监控（可选，在另一个终端）

```bash
python monitor_training.py
```

### 3. 训练后处理

训练结束后，运行以下命令提取结果：

```bash
# 提取最好的10次训练结果
python process_training_results.py <result_dir>

# 提取最好的5个检查点
python manage_checkpoints.py <result_dir>
```

---

## 主要改进功能

### ✅ 1. 训练轮数输出

训练过程中会在终端输出训练轮数，每轮之间用分隔线：

```
====================================================================================================
训练轮数: 1 | 平均奖励: -5.23 | 最高奖励: -3.98 | 最低奖励: -8.28
====================================================================================================
```

### ✅ 2. 最好的10次结果记录

训练结束后，运行 `process_training_results.py` 会生成 `training_logs/best_results.json`，包含：
- 排名
- 训练轮数
- 奖励
- 保护目标数量
- 拦截导弹数量

### ✅ 3. 最好的5个检查点保存

运行 `manage_checkpoints.py` 会：
- 从所有检查点中找出最好的5个
- 复制到 `checkpoints/best_5/` 目录
- 生成 `checkpoints/checkpoint_info.json` 记录信息

### ✅ 4. 实时绘图监控

运行 `monitor_training.py` 会显示4个实时图表：
- 奖励变化曲线
- 保护目标存活数
- 拦截导弹数量
- 最近10次训练结果

### ✅ 5. 冷却机制优化

- **基础冷却期**：从3步改为1步
- **动态调整**：高威胁目标冷却期=0，可以立即再次拦截
- **强制拦截**：威胁度>=8.0或距离<25km时，即使冷却期未到也强制拦截

### ✅ 6. 输出优化

- 前100步不输出详细信息（减少日志噪音）
- 保留启动和加载信息
- 100步之后正常输出

---

## 文件说明

### 核心文件

- `main_train.py`：训练主程序（已修改）
- `env_sdfk.py`：环境文件（已修改，改进冷却机制）

### 新增文件

- `training_monitor.py`：训练监控模块（基础框架）
- `process_training_results.py`：提取最好的10次结果
- `monitor_training.py`：实时监控脚本
- `manage_checkpoints.py`：检查点管理脚本

### 输出文件

- `training_logs/best_results.json`：最好的10次训练结果
- `checkpoints/best_5/`：最好的5个检查点
- `checkpoints/checkpoint_info.json`：检查点信息

---

## 关键改进说明

### 冷却机制改进（最重要）

**问题**：之前固定冷却期3步，导致很多目标在冷却期无法拦截

**解决方案**：
1. 基础冷却期从3步改为1步
2. 动态冷却：高威胁目标冷却期=0
3. 强制拦截：高威胁时忽略冷却期

**预期效果**：
- 高威胁目标可以立即再次拦截
- 减少"跳过重复拦截目标，仍在冷却期"的情况
- 保护目标存活率显著提升

---

## 故障排除

### 问题1：回调不工作

如果训练轮数没有输出，可能是Ray Tune版本问题。可以：
1. 检查Ray版本：`pip show ray`
2. 查看训练日志文件（通常在ray_results目录下）

### 问题2：监控脚本找不到结果目录

手动指定结果目录：
```bash
python monitor_training.py "C:/Users/yourname/ray_results/your_trial_dir"
```

### 问题3：效果仍不理想

1. 检查冷却机制是否生效（查看日志中是否还有大量"跳过"信息）
2. 进一步降低冷却期（修改 `base_cooldown_steps = 0`）
3. 增加训练轮数
4. 调整奖励函数参数

---

## 下一步优化建议

如果训练效果仍不理想，可以：

1. **进一步优化冷却机制**：
   - 将 `base_cooldown_steps` 设为0
   - 或者完全移除冷却机制

2. **调整奖励参数**：
   ```python
   env_config = {
       'intercept_failure_penalty': 5.0,  # 增大失败惩罚
       'intercept_success_reward': 10.0,  # 增大成功奖励
   }
   ```

3. **增加训练轮数**：
   - 建议至少10000轮

4. **改进观测空间**：
   - 添加更详细的目标信息
   - 添加拦截历史信息
