# 首都防空强化学习设计分析报告（最新）

> 目标：让红方保护目标存活更多，减小导弹浪费，并在多目标来袭时保持及时、高效的分层拦截。

---

## 1. 现有强化学习设计概览

### 1.1 动作空间
- 类型：`MultiDiscrete`，总维度 73  
  - 前 72 维：24 个目标 × 3 种防空单元（C-400 / HQ-9A / HQ-12），每维取值 0~3 表示发射 0~3 枚。  
  - 最后 1 维：全局 do-nothing 开关（0 执行动作；1/2 表示跳过）。  
- 解析：在 `_parse_action` 中按威胁排序的目标依序映射到 24 行动作矩阵，生成 `engagement_plan`。

### 1.2 观测空间（20 维）
- 蓝方导弹信息（7 维）：对所有探测导弹取平均的类型/经纬度/速度/航向/识别状态/高度。  
- 红方防空单元状态（9 维）：C-400、HQ-9A、HQ-12 各 3 维（剩余导弹比例、平均经度、平均纬度）。  
- 时间进度（4 维）：基于想定时间的 4 个归一化特征。

### 1.3 奖励设计
- 拦截成功奖励：`intercept_success_reward`（默认 5.0），早期拦截有 `early_intercept_bonus`（默认 2.0）。  
- 拦截失败惩罚：距离 <30km 未拦截按距离加倍惩罚，`intercept_failure_penalty` 默认 3.0。  
- 保护目标惩罚：动态递增惩罚（50, 60, 70…），全部被摧毁额外 -500。  
- 存活奖励：有威胁时，存活目标每步 0.3。  
- 拦截率奖励：拦截率×5.0。  
- 发射奖励：C-400 发 1 枚 +0.1；HQ-9A/HQ-12 发 ≥2 枚 +0.1。  
- 距离塑形：近距离 +0.05，过远 -0.05，早期拦截额外 +`early_intercept_bonus`×0.5。

### 1.4 策略与控制逻辑
- 目标排序：按威胁度（距离、速度、射程可达性、ETA）排序。  
- 单元选择：在 `_select_best_unit_for_engagement` 中按导弹充足度与距离评分选最优单元；若不可用，尝试 `_find_alternative_unit`。  
- 冷却机制：基础冷却 1 步；高威胁/近距离可动态降到 0，低威胁可延长；威胁 >=8 或距离 <25km 时强制允许再次拦截。  
- 末端检测：在 step 内更新态势前后检测拦截成功/失败，追加塑形奖励。  
- 装弹记录：`unit_reload_time` 仅记录发射历史；实际装填由墨子内部管理。

---

## 2. 当前问题对照与原因分析

> 下列问题均来自你反馈的训练表现和终端输出。

1) **开局 C-400 发射过多，导弹浪费**  
   - 原因：未对单目标 C-400 同发数量做上限；动作空间允许高值，且奖励对 HQ-9A/HQ-12 较弱时，策略倾向远程大弹。

2) **后期 HQ-9A / HQ-12 发射太少或太慢**  
   - 原因：  
     - 虽已修正 HQ-12 不开火的 bug，但缺少末端“必须出手”的下限约束。  
     - 冷却/分配与奖励对末端拦截的激励仍不足。  
     - 多威胁并行时没有额外鼓励多平台协同。

3) **敌方导弹密集且逼近时，有时不发或只发少量导弹**  
   - 原因：策略在密集来袭场景缺乏“补齐最少弹量”的约束；惩罚主要发生在目标被毁时，导致临界阶段仍可能保守。

4) **多平台协同不足**  
   - 原因：动作空间虽支持多平台，但缺少鼓励多平台齐射的奖励，且没有对“未分配”情况施加即时惩罚。

---

## 3. 已做的关键改进（本次代码调整）

1. **防止 C-400 过度开火（浪费）**  
   - 新增 `max_c400_volley`（默认 2），对单目标 C-400 同发数量做上限控制。

2. **末端必须出手的下限约束**  
   - `min_hq_volley_under_threat`（默认 1）：当目标距离 <60km 且 HQ-9A/HQ-12 本次未分配时，至少补 1 枚（防止“零拦截”）。  
   - 对高威胁但未分配的目标给予惩罚 `under_engage_penalty`（默认 0.5）。

3. **密集来袭时的协同奖励**  
   - 当空中威胁数量 ≥ `mass_threat_bonus_threshold`（默认 15）且 HQ-9A/HQ-12 有发射时，额外奖励 `mass_threat_bonus`（默认 0.2），鼓励多平台齐射。

4. **HQ-12 弹量逻辑已修正**  
   - 移除将 HQ-12 可用弹量直接减 24 的错误逻辑，避免被判定无弹而不打。

> 以上改动均已写入 `env_sdfk.py`：新增配置、C-400 上限控制、HQ-9A/HQ-12 末端下限、密集来袭奖励、未分配惩罚。

---

## 4. 仍存在的局限与建议（可选后续优化）

1) **观测信息仍较粗糙**  
   - 只用均值特征，缺少“最危险若干目标”的精细特征；可扩展 3~5 个 Top 威胁的距离/速度/ETA/是否在各射程内等。

2) **动作空间仍大（固定 24×3 槽位）**  
   - 可考虑更紧凑的动作表示：先选目标，再给三类单元的发射数，或对真实探测数量做裁剪。

3) **奖励塑形进一步细化**  
   - 对“未分配且距离 < X”增加递增惩罚；对“多平台协同同一目标”可加轻微奖励（避免全部压到 C-400）。

4) **装填/补装精细模拟**  
   - 目前仅记录发射历史；如需更严格的补装窗口，可结合墨子接口读取真实装填状态（若可获得）。

---

## 5. 针对四个问题的代码级改进建议（已部分落地）

### 5.1 开局 C-400 过度发射
- 已落地：`max_c400_volley` 控制单目标上限（默认 2）。  
- 可选：对同一步总 C-400 发射量加全局上限，或在奖励中对超量发射增加小惩罚。

### 5.2 后期 HQ-9A / HQ-12 发射少或慢
- 已落地：目标 <60km 且未分配时强制至少 1 发（`min_hq_volley_under_threat`）。  
- 建议：  
  - 加强近程发射奖励（可提高 `fire_reward_near`），或增加“末端拦截成功”额外奖励。  
  - 在距离阈值内，对 HQ-9A/HQ-12 发射不足的情况施加更大惩罚。

### 5.3 密集来袭不出手
- 已落地：威胁总数 ≥15 时，HQ-9A/HQ-12 发射获得协同奖励 `mass_threat_bonus`。  
- 建议：  
  - 将阈值下调（如 10~12）并稍增奖励，以强化群攻场景下的出手积极性。  
  - 对高威胁且未分配的目标增加阶梯惩罚。

### 5.4 多平台协同
- 已落地：群攻奖励鼓励多平台齐射。  
- 建议：  
  - 对“同一目标由两类平台同时拦截”增加小额奖励，鼓励远/中/近层协同。  
  - 记录并统计多平台协同的命中率，作为额外指标反馈。

---

## 6. 操作指引（训练与监控）

1. 训练：  
   ```bash
   python mozi_ai_sdk/sdfk_test/main_train.py --platform_mode eval --training_iteration 1000
   ```
2. 实时监控（若需要）：  
   ```bash
   python mozi_ai_sdk/sdfk_test/monitor_training.py  # 或指定结果目录
   ```
3. 训练后提取最佳结果/检查点：  
   ```bash
   python mozi_ai_sdk/sdfk_test/process_training_results.py <result_dir>
   python mozi_ai_sdk/sdfk_test/manage_checkpoints.py <result_dir>
   ```

---

## 7. 结论

- 造成 HQ-12 不打的核心 bug 已修复；当前策略仍偏重 C-400 的根源在于缺乏末端“必须出手”的硬约束与密集来袭的协同激励。  
- 本次改动增加了：  
  - C-400 单目标发射上限  
  - HQ-9A/HQ-12 近程最小火力约束  
  - 高威胁未分配惩罚  
  - 群攻场景多平台协同奖励  
- 预期效果：减少 C-400 浪费，增加 HQ-9A/HQ-12 末端拦截积极性，密集来袭时更容易多平台齐射，从而提升保护目标存活率。  
若后续仍有不足，可进一步细化奖励、丰富观测特征、压缩动作空间并加强末端拦截惩罚。

---

## 8. 上海防空想定下的新增问题与改进方向

> 上海想定相对北京的变化：  
> - 蓝方舰船每轮固定两波齐射，且每波在三条航线中随机选择一条；  
> - 蓝方飞机每轮 1~2 波攻击，波数随机；  
> - 红方增加电子干扰手段；  
> - 红方 C-400/HQ-9A/HQ-12 与保护目标的空间布局整体南移到华东沿海；  
> - C-400 单平台弹量增到 80，HQ-9A 增到 64，HQ-12 为 36。

### 8.1 策略设计在新想定中的不妥之处

1) **奖励函数没有体现“经济性目标”**  
- 目前奖励主导仍是：保护目标存活 + 拦截成功率；  
- 导弹消耗只通过极小的发射奖励/惩罚间接影响，且对 C-400 与 HQ-9A/HQ-12 没有明显的差异成本；  
- 在导弹总量翻倍后，智能体几乎“感受不到”多打一发的成本，因此会倾向于用火力换安全，难以学出“少耗弹但同样保住目标”的策略。

2) **多波次/多航线攻击下，缺少“弹药留存意识”**  
- 蓝方舰船与飞机采用多波次进攻，第一波并非全部火力；  
- 现有奖励没有考虑“后续波次预计威胁”，也没有惩罚“前几波打空后波无弹可用”的局面；  
- 这会导致：前几波拦截时策略仍然可能较激进，消耗过多 C-400 / HQ-9A，导致后面波次防空能力下降。

3) **电子干扰效果未纳入观测/奖励**  
- 代码层面虽然环境中加入了电子干扰，但观测状态中没有“干扰是否开启/生效”的显式特征；  
- 奖励中也没有鼓励“在干扰效果好时适当少发弹”，或“在干扰失效区更积极拦截”；  
- 结果是：干扰对策略来说更像是“背景噪声”，而非可利用的战术资源。

4) **多航线/多方向威胁下的资源分配不够精细**  
- 现有观测仍是全场平均特征 + 各类单元平均位置；  
- 上海想定中，敌方航线更分散（多个方向/高度），但智能体无法精确区分“某个方向威胁更大、需要优先守护哪一片保护区”；  
- 导致部分场景下出现：某方向威胁较集中，但火力仍较平均分散，局部防空过载。

5) **与“无策略基线”相比，尚未显式对齐评估指标**  
- 你的评估标准是：在同样蓝方攻势下，**保护目标存活更多、红方耗弹更少** 相比“见弹就打”的基线；  
- 但当前训练奖励与这一评估公式并未严格一致：  
  - 保护目标损失惩罚足够大，这点符合；  
  - 但导弹消耗成本未显式引入，容易出现“生存好、耗弹也高”的策略，而这在你期望的指标下不一定是“更优”。

### 8.2 针对上海想定的奖励改进建议

1) **引入显式“导弹成本”项，使奖励与评估指标对齐**  
建议在 `_update` / `_get_win_score` 的基础上增加一项：

- 记每步发射的 C-400/HQ-9A/HQ-12 数量分别为 \(n_{c400}, n_{hq9}, n_{hq12}\)，对应单位成本为 \(c_{c400}, c_{hq9}, c_{hq12}\)（可直接用 `RED_MISSILE_COST`）。  
- 新增成本惩罚：
  \[
  R_{\text{cost}} = - \alpha \cdot (c_{c400} n_{c400} + c_{hq9} n_{hq9} + c_{hq12} n_{hq12})
  \]
  其中 \(\alpha\) 为缩放系数（如 0.01~0.05），通过经验调节。  
- 这样，策略在“多打一发导弹”时会有可感知的即时成本，能在“拦截成功”和“少耗弹”之间自动权衡。

2) **分层设定不同平台的相对成本权重**  
- C-400 代表远程/价格昂贵，应设定更高成本权重；  
- HQ-12 更适合作为末端兜底，可设较低成本权重；  
- 通过这种“价格差”引导策略优先使用中/近程弹，在不得已时才大量动用 C-400。

3) **为“保留弹药以应对后续波次”增加软约束**  
可以基于当前时间进度与剩余弹药占比构造一项：

- 若时间尚早（如 ETA/时刻 < 全局 1/2），但某类平台弹药已消耗超过阈值（例如 >70%），则给一个小惩罚；  
- 反之，在接近尾声时弹药仍大量剩余且拦截效果不佳，则也可给出轻微惩罚，鼓励“适度使用、不过度保守”。

4) **为“多平台协同但不过度堆弹”设计更细的奖励**  
- 当同一威胁由 C-400 + HQ-9A/HQ-12 分层拦截且总发射量在合理区间（如 2~4 枚）时，给一点额外奖励；  
- 若单一目标在短时间被发射过多枚（明显超出合理需求），则增加小惩罚，约束“过度覆盖”。

### 8.3 观测空间针对上海想定的增强建议

1) **加入波次/时间段信息**  
- 可在时间特征中引入更明确的波次阶段，例如：  
  - 第 1 波舰船导弹进行中/结束；  
  - 飞机导弹首波/次波进行中；  
- 若无法直接获取，可用时间区间 + 已拦截数的拐点来粗略估计。

2) **加入方向/扇区级别的威胁统计**  
- 将整片空域按几个扇区（如东北/东南/南）划分，对每个扇区统计：  
  - 当前导弹数量  
  - 平均距离/ETA  
  - 是否主要威胁某一批保护目标  
- 这样策略可以学会“对某个扇区集中防守”，而不是平均摊派。

3) **如有可能，加入电子干扰状态**  
- 如果环境中能提供“干扰是否开启/正在作用于哪些导弹”的标志，可将其编码进观测；  
- 奖励中则对“在干扰有效区域内少发弹”给予微弱正反馈，引导策略利用干扰效果节省弹药。

---

### 8.4 总结：如何让上海想定下的模型优于“见弹就打”基线

要实现你希望的对比效果（**保护目标不逊于基线，同时耗弹更少**），关键是：

1. **奖励函数必须显式体现“导弹成本”**，且不同平台成本有差异；  
2. **分层/多波次信息需在观测中可见**，策略才能学会“保留弹药、关键时刻再用”；  
3. 在此基础上，继续保留我们之前为北京想定设计的：  
   - 保护目标损失的强烈惩罚；  
   - 拦截成功/早期拦截奖励；  
   - 高威胁未拦截惩罚；  
   - 群攻场景下多平台协同奖励。

这样一来，训练出来的策略在“保护目标数量”和“耗弹总数”两个维度上，都有机会明显优于“看到导弹就随便打”的简单基线。 
